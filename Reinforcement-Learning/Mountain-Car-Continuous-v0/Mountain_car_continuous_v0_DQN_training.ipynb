{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Net(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=21, bias=True)\n",
      "  )\n",
      ")\n",
      "no of iterations         mean reward:\n",
      "       1             -0.9990000000000008\n",
      "       2             -48.95099999999961\n",
      "       3             -48.95099999999961\n",
      "       4             -48.95099999999961\n",
      "       5             -48.95099999999961\n",
      "       6             -48.95099999999961\n",
      "       7             -54.93899999999978\n",
      "       8             -58.211999999999875\n",
      "       9             -59.6684999999999\n",
      "       10             -60.44999999999992\n",
      "       11             -61.23899999999996\n",
      "       12             -62.7255\n",
      "       13             -62.342999999999996\n",
      "       14             -62.389499999999984\n",
      "       15             -61.978499999999976\n",
      "       16             -61.78499999999997\n",
      "       17             -61.775999999999975\n",
      "       18             -61.060499999999955\n",
      "       19             -57.55169999999988\n",
      "       20             68.08570000000009\n",
      "       21             -43.69499999999961\n",
      "       22             -41.536499999999585\n",
      "       23             -41.3750999999996\n",
      "       24             84.98260000000003\n",
      "       25             -37.09569999999964\n",
      "       26             -49.59149999999962\n",
      "       27             33.05380000000021\n",
      "       28             -51.35139999999975\n",
      "       29             -46.693900000000134\n",
      "       30             -43.212000000000145\n",
      "       31             -48.16950000000013\n",
      "       32             -55.03249999999999\n",
      "       33             -54.46699999999991\n",
      "       34             -54.51199999999987\n",
      "       35             -52.36499999999982\n",
      "       36             -46.76199999999977\n",
      "       37             -43.78699999999989\n",
      "       38             -39.43339999999985\n",
      "       39             -28.992200000000174\n",
      "       40             -23.807000000000194\n",
      "       41             -20.567500000000106\n",
      "       42             -9.909499999999941\n",
      "       43             -4.319999999999977\n",
      "       44             -4.082999999999995\n",
      "       45             -4.729499999999957\n",
      "       46             -5.6578999999999455\n",
      "       47             -6.8839999999999595\n",
      "       48             -7.99779999999995\n",
      "       49             -8.612899999999952\n",
      "       50             -8.75349999999996\n",
      "       51             -9.151899999999968\n",
      "       52             -9.507299999999969\n",
      "       53             -9.327599999999961\n",
      "       54             -8.69019999999995\n",
      "       55             -8.594399999999947\n",
      "       56             -7.927899999999942\n",
      "       57             -8.07209999999995\n",
      "       58             -7.782299999999944\n",
      "       59             -8.623499999999943\n",
      "       60             -8.660699999999952\n",
      "       61             -9.010999999999957\n",
      "       62             -9.906499999999955\n",
      "       63             -10.130299999999972\n",
      "       64             -10.524399999999982\n",
      "       65             -11.059599999999985\n",
      "       66             -10.747399999999985\n",
      "       67             -11.305299999999999\n",
      "       68             -11.832700000000015\n",
      "       69             -11.557000000000006\n",
      "       70             -11.874400000000009\n",
      "       71             -11.763500000000006\n",
      "       72             -12.507499999999975\n",
      "       73             -12.44729999999998\n",
      "       74             -13.257899999999958\n",
      "       75             -11.839699999999958\n",
      "       76             -11.247299999999962\n",
      "       77             -10.662299999999957\n",
      "       78             -10.89829999999996\n",
      "       79             -11.885399999999972\n",
      "       80             -18.206600000000137\n",
      "       81             -38.75519999999958\n",
      "       82             -31.691500000000083\n",
      "       83             -34.82150000000006\n",
      "       84             -44.19649999999998\n",
      "       85             -38.425000000000004\n",
      "       86             -51.70499999999997\n",
      "       87             -23.643000000000168\n",
      "       88             -19.630000000000056\n",
      "       89             -33.648999999999994\n",
      "       90             -39.75099999999988\n",
      "       91             -43.07949999999981\n",
      "       92             -47.4944999999998\n",
      "       93             -52.2334999999998\n",
      "       94             -49.710599999999815\n",
      "       95             -42.023099999999815\n",
      "       96             -37.58309999999983\n",
      "       97             -20.676900000000064\n",
      "       98             -17.53870000000005\n",
      "       99             -14.872100000000032\n",
      "       100             -10.979100000000079\n",
      "       101             0.0\n",
      "       102             0.0\n",
      "       103             -0.06680000000000005\n",
      "       104             -16.952399999999866\n",
      "       105             -20.267399999999952\n",
      "       106             -20.14809999999996\n",
      "       107             -20.40009999999997\n",
      "       108             -20.647599999999983\n",
      "       109             -25.62350000000027\n",
      "       110             -27.45490000000043\n",
      "       111             -39.361200000000316\n",
      "       112             -32.02210000000021\n",
      "       113             -32.34430000000027\n",
      "       114             -27.313400000000406\n",
      "       115             -31.260600000000387\n",
      "       116             -37.633200000000365\n",
      "       117             -39.266100000000336\n",
      "       118             -39.24480000000024\n",
      "       119             -43.56480000000021\n",
      "       120             82.65530000000004\n",
      "       121             82.56350000000002\n",
      "       122             76.71690000000005\n",
      "       123             -14.687399999999897\n",
      "       124             -5.563799999999986\n",
      "       125             -10.456499999999926\n",
      "       126             -42.259199999999566\n",
      "       127             -43.15619999999956\n",
      "       128             -43.30979999999955\n",
      "       129             -22.57739999999993\n",
      "       130             -15.984000000000012\n",
      "       131             -15.984000000000012\n",
      "       132             -15.984000000000012\n",
      "       133             -15.984000000000012\n",
      "       134             -15.984000000000012\n",
      "       135             -15.984000000000012\n",
      "       136             -15.984000000000012\n",
      "       137             -15.984000000000012\n",
      "       138             -15.984000000000012\n",
      "       139             -15.984000000000012\n",
      "       140             -15.984000000000012\n",
      "       141             -15.984000000000012\n",
      "       142             -15.984000000000012\n",
      "       143             -15.984000000000012\n",
      "       144             -15.984000000000012\n",
      "       145             -15.984000000000012\n",
      "       146             -15.984000000000012\n",
      "       147             -15.984000000000012\n",
      "       148             -15.984000000000012\n",
      "       149             -15.984000000000012\n",
      "       150             25.469999999999988\n",
      "       151             46.97479999999998\n",
      "problem solved with number of iterations and best mean reward: \n",
      "       152             90.51979999999999\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is a simple implementation of deep q-network (DQN) for mountain car \n",
    "(MountainCarContinuous-v0) modlue from OpenAI Gym\n",
    "'''\n",
    "import gym\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.9\n",
    "ACTION_SIZE = 21\n",
    "MAX_EPISODES = 10\n",
    "\n",
    "# neural network model \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, ACTION_SIZE):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, ACTION_SIZE)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) \n",
    "\n",
    "def state_action(net, obs, act): \n",
    "    obs_v = torch.FloatTensor([obs])\n",
    "    value = net(obs_v) \n",
    "    act_index = int(float(ACTION_SIZE/2)*act[0])+int(ACTION_SIZE/2) \n",
    "    v = value[0][act_index] \n",
    "    return v \n",
    "\n",
    "def evaluate(batch, net, net_target): \n",
    "    value = 0.0 \n",
    "    for state, next_state, action, reward, done in batch:\n",
    "        if done == False: \n",
    "            v = state_action(net,state,action) \n",
    "            state_v = torch.FloatTensor([next_state])\n",
    "            target = net_target(state_v)  \n",
    "            index = target.max(dim = -1)[1]\n",
    "            vv_target = target[0][index] \n",
    "            v_target = vv_target[0]\n",
    "            difference = v - reward-GAMMA*v_target\n",
    "            value += difference*difference \n",
    "        else: \n",
    "            v = state_action(net,state,batch_action) \n",
    "            difference = v-reward \n",
    "            value += difference*difference \n",
    "    return value\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "    net = Net(env.observation_space.shape[0], ACTION_SIZE) \n",
    "    net_target = Net(env.observation_space.shape[0], ACTION_SIZE) \n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    print(net)\n",
    "#   prepare initial training batch\n",
    "    batch_state = [] \n",
    "    batch_next_state = []\n",
    "    batch_reward = [] \n",
    "    batch_action = []\n",
    "    batch_done = []\n",
    "    total_reward = 0.0 \n",
    "    act = np.zeros((1,),dtype = float) \n",
    "    no_iter = 0 \n",
    "    print(\"no of iterations         mean reward:\")\n",
    "    no_done = 0 \n",
    "    obs = env.reset() \n",
    "    while no_done < MAX_EPISODES: \n",
    "        batch_state.append(obs) \n",
    "        state_v = torch.FloatTensor([obs])\n",
    "        target = net_target(state_v)  \n",
    "        index = target.max(dim = -1)[1] \n",
    "        index_v = index.numpy() \n",
    "        act[0] = (index_v-int(ACTION_SIZE/2))/float(int(ACTION_SIZE/2))\n",
    "        batch_action.append(act)\n",
    "        obs, reward, done, _ = env.step(act) \n",
    "        batch_next_state.append(obs)\n",
    "        batch_reward.append(reward)\n",
    "        batch_done.append(done)\n",
    "        if done: \n",
    "            no_done += 1\n",
    "            obs = env.reset()\n",
    "    batch = list(zip(batch_state, batch_next_state, batch_action,batch_reward, \n",
    "                                       batch_done))\n",
    "    \n",
    "    # start traing\n",
    "    batch_state_t = [] \n",
    "    batch_next_state_t = []\n",
    "    batch_reward_t = [] \n",
    "    batch_action_t = []\n",
    "    batch_done_t = []\n",
    "    while True:  \n",
    "        batch_total_reward = 0.0\n",
    "        total_reward = 0.0 \n",
    "        obs = env.reset() \n",
    "        no_done = 0\n",
    "        while no_done < MAX_EPISODES: \n",
    "            batch_state_t.append(obs) \n",
    "            state_v = torch.FloatTensor([obs])\n",
    "            target = net_target(state_v)  \n",
    "            index = target.max(dim = -1)[1] \n",
    "            index_v = index.numpy() \n",
    "            act[0] = (index_v-int(ACTION_SIZE/2))/float(int(ACTION_SIZE/2))\n",
    "            batch_action_t.append(act)\n",
    "            obs, reward, done, _ = env.step(act) \n",
    "            batch_next_state_t.append(obs)\n",
    "            batch_reward_t.append(reward)\n",
    "            batch_done_t.append(done) \n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                no_done += 1\n",
    "                obs = env.reset() \n",
    "                batch_total_reward += total_reward\n",
    "                total_reward = 0.0 \n",
    "        if len(batch_done_t) > BATCH_SIZE:\n",
    "            batch_t = random.sample(list(zip(batch_state_t, batch_next_state_t, batch_action_t,\n",
    "                                             batch_reward_t, batch_done_t)), BATCH_SIZE) \n",
    "            size = BATCH_SIZE \n",
    "        else: \n",
    "            batch_t = list(zip(batch_state_t, batch_next_state_t, batch_action_t,batch_reward_t,\n",
    "                               batch_done_t))\n",
    "            size = len(batch_done_t)\n",
    "        \n",
    "        del batch[len(batch)-size:]\n",
    "        for b in batch_t: \n",
    "            batch.append(b)\n",
    "\n",
    "        mean = batch_total_reward/no_done\n",
    "        if mean > 90: \n",
    "            torch.save(net.state_dict(), \"best_solution_for_mountain_car_continuous_v0.data\")\n",
    "            no_iter = no_iter+1\n",
    "            print(\"problem solved with number of iterations and best mean reward: \") \n",
    "            print(\"      \",no_iter, \"           \",mean) \n",
    "            break\n",
    "        no_iter = no_iter+1\n",
    "        print(\"      \",no_iter, \"           \",mean) \n",
    "        batch_t = random.sample(batch, size)\n",
    "        optimizer.zero_grad() \n",
    "        loss_t = evaluate(batch_t, net, net_target)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()                                 \n",
    "        net_target.load_state_dict(net.state_dict()) \n",
    "        batch_state_t = []\n",
    "        batch_next_state_t = []\n",
    "        batch_reward_t = [] \n",
    "        batch_action_t = []\n",
    "        batch_done_t = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
